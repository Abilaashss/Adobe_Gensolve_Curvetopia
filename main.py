# -*- coding: utf-8 -*-
"""Adobe_regularise_GAn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EEqsiS4UyvzoB0cH7t9wX3og6FhulqXa
"""

import zipfile
import os
from PIL import Image
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Reshape, LeakyReLU, Conv2DTranspose, Conv2D, BatchNormalization, Activation, Input, Flatten, Dropout
from tensorflow.keras.optimizers import Adam
from matplotlib import pyplot as plt

def load_images_from_zip(zip_file_path, target_size=(64, 64)):
    images = []
    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
        for file_name in zip_ref.namelist():
            if file_name.lower().endswith(('.png', '.jpg', '.jpeg')):
                with zip_ref.open(file_name) as file:
                    img = Image.open(file).convert('RGB')
                    img = img.resize(target_size)
                    img_array = np.array(img)
                    images.append(img_array)
    return np.array(images)

x_train_zip = '/content/x_train.zip'
y_train_zip = '/content/y_train.zip'
x_test_zip = '/content/x_test.zip'
y_test_zip = '/content/y_test.zip'

# Load and preprocess data
def load_and_preprocess_data(x_train_zip, y_train_zip, x_test_zip, y_test_zip, target_size=(64, 64)):
    x_train = load_images_from_zip(x_train_zip, target_size)
    y_train = load_images_from_zip(y_train_zip, target_size)
    x_test = load_images_from_zip(x_test_zip, target_size)
    y_test = load_images_from_zip(y_test_zip, target_size)

    # Normalize the images
    x_train = (x_train.astype('float32') - 127.5) / 127.5
    y_train = (y_train.astype('float32') - 127.5) / 127.5
    x_test = (x_test.astype('float32') - 127.5) / 127.5
    y_test = (y_test.astype('float32') - 127.5) / 127.5

    return x_train, y_train, x_test, y_test

# Define the Generator
def build_generator(latent_dim, channels=3):
    inputs = Input(shape=(64, 64, channels))

    # Encoder
    x = Conv2D(64, kernel_size=3, strides=2, padding='same')(inputs)
    x = LeakyReLU(alpha=0.2)(x)
    x = Conv2D(128, kernel_size=3, strides=2, padding='same')(x)
    x = LeakyReLU(alpha=0.2)(x)
    x = Conv2D(256, kernel_size=3, strides=2, padding='same')(x)
    x = LeakyReLU(alpha=0.2)(x)
    x = Conv2D(512, kernel_size=3, strides=2, padding='same')(x)
    x = LeakyReLU(alpha=0.2)(x)

    x = Flatten()(x)
    x = Dense(latent_dim)(x)

    # Decoder
    x = Dense(4 * 4 * 512)(x)
    x = Reshape((4, 4, 512))(x)
    x = Conv2DTranspose(256, kernel_size=4, strides=2, padding='same')(x)
    x = LeakyReLU(alpha=0.2)(x)
    x = Conv2DTranspose(128, kernel_size=4, strides=2, padding='same')(x)
    x = LeakyReLU(alpha=0.2)(x)
    x = Conv2DTranspose(64, kernel_size=4, strides=2, padding='same')(x)
    x = LeakyReLU(alpha=0.2)(x)
    x = Conv2DTranspose(channels, kernel_size=4, strides=2, padding='same', activation='tanh')(x)

    return Model(inputs, x)

def build_discriminator(img_shape):
    inputs = Input(shape=img_shape)

    x = Conv2D(64, kernel_size=3, strides=2, padding='same')(inputs)
    x = LeakyReLU(alpha=0.2)(x)
    x = Conv2D(128, kernel_size=3, strides=2, padding='same')(x)
    x = LeakyReLU(alpha=0.2)(x)
    x = Conv2D(256, kernel_size=3, strides=2, padding='same')(x)
    x = LeakyReLU(alpha=0.2)(x)
    # x = Conv2D(512, kernel_size=3, strides=2, padding='same')(x)
    # x = LeakyReLU(alpha=0.2)(x)

    x = Flatten()(x)
    x = Dropout(0.4)(x)
    x = Dense(1, activation='sigmoid')(x)

    return Model(inputs, x)

def build_gan(generator, discriminator):
    discriminator.trainable = False
    gan_input = Input(shape=(64, 64, 3))
    x = generator(gan_input)
    gan_output = discriminator(x)
    return Model(gan_input, gan_output)

def train_gan(generator, discriminator, gan, x_train, y_train, epochs, batch_size):
    for epoch in range(epochs):
        # Train Discriminator
        idx = np.random.randint(0, x_train.shape[0], batch_size)
        real_imgs = y_train[idx]
        fake_imgs = generator.predict(x_train[idx])

        d_loss_real = discriminator.train_on_batch(real_imgs, np.ones((batch_size, 1)))
        d_loss_fake = discriminator.train_on_batch(fake_imgs, np.zeros((batch_size, 1)))
        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

        # Train Generator
        idx = np.random.randint(0, x_train.shape[0], batch_size)
        g_loss = gan.train_on_batch(x_train[idx], np.ones((batch_size, 1)))

        if epoch % 10 == 0:
            print(f"Epoch {epoch}, D Loss: {d_loss[0]}, G Loss: {g_loss}")

def visualize_results(generator, x_test, y_test, num_samples=5):
    idx = np.random.randint(0, x_test.shape[0], num_samples)
    irregular_imgs = x_test[idx]
    regular_imgs = y_test[idx]
    generated_imgs = generator.predict(irregular_imgs)

    plt.figure(figsize=(15, 5))
    for i in range(num_samples):
        # Irregular image
        plt.subplot(3, num_samples, i + 1)
        plt.imshow((irregular_imgs[i] * 0.5 + 0.5).clip(0, 1))
        plt.axis('off')
        if i == 0:
            plt.title('Irregular')

        # Regular image
        plt.subplot(3, num_samples, i + 1 + num_samples)
        plt.imshow((regular_imgs[i] * 0.5 + 0.5).clip(0, 1))
        plt.axis('off')
        if i == 0:
            plt.title('Regular')

        # Generated image
        plt.subplot(3, num_samples, i + 1 + 2 * num_samples)
        plt.imshow((generated_imgs[i] * 0.5 + 0.5).clip(0, 1))
        plt.axis('off')
        if i == 0:
            plt.title('Generated')

    plt.tight_layout()
    plt.show()

# Main execution
def main():
    # Specify the paths to your zip files
    x_train_zip = '/content/x_train.zip'
    y_train_zip = '/content/y_train.zip'
    x_test_zip = '/content/x_test.zip'
    y_test_zip = '/content/y_test.zip'

    # Load and preprocess data
    x_train, y_train, x_test, y_test = load_and_preprocess_data(x_train_zip, y_train_zip, x_test_zip, y_test_zip)

    print("Data shapes:")
    print(f"x_train: {x_train.shape}")
    print(f"y_train: {y_train.shape}")
    print(f"x_test: {x_test.shape}")
    print(f"y_test: {y_test.shape}")

    # Build and compile models
    img_shape = (64, 64, 3)
    latent_dim = 100

    generator = build_generator(latent_dim)
    discriminator = build_discriminator(img_shape)
    discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])
    gan = build_gan(generator, discriminator)
    gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0001, 0.5))

    # Train the GAN
    epochs = 100
    batch_size = 32
    train_gan(generator, discriminator, gan, x_train, y_train, epochs, batch_size)

    # Visualize results
    visualize_results(generator, x_test, y_test)

if __name__ == "__main__":
    main()



# Now you have your images loaded into numpy arrays
print(f'x_train shape: {x_train.shape}')
print(f'y_train shape: {y_train.shape}')
print(f'x_test shape: {x_test.shape}')
print(f'y_test shape: {y_test.shape}')

# from keras.datasets.cifar10 import load_data
from matplotlib import pyplot as plt

# (x_train, y_train), (x_test, y_test) = load_data()

for i in range(45):
  plt.subplot(9, 5, i+1)
  plt.axis('off')
  plt.imshow(x_train[i])

plt.show()

print(x_train.shape)

# Define the Generator model
def define_generator():
    model = Sequential()
    model.add(Dense(256 * 4 * 4, input_dim=32*32*3))
    model.add(LeakyReLU(alpha=0.2))
    model.add(Reshape((4, 4, 256)))

    model.add(Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))
    model.add(LeakyReLU(alpha=0.2))

    model.add(Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))
    model.add(LeakyReLU(alpha=0.2))

    model.add(Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))
    model.add(LeakyReLU(alpha=0.2))

    model.add(Conv2D(3, (3, 3), activation='tanh', padding='same'))

    return model

def define_discriminator(input_shape=(32, 32, 3)):
    model = Sequential()
    model.add(Conv2D(64, (3, 3), padding='same', input_shape=input_shape))
    model.add(LeakyReLU(alpha=0.2))

    model.add(Conv2D(128, (3, 3), strides=(2, 2), padding='same'))
    model.add(LeakyReLU(alpha=0.2))

    model.add(Conv2D(128, (3, 3), strides=(2, 2), padding='same'))
    model.add(LeakyReLU(alpha=0.2))

    model.add(Conv2D(256, (3, 3), strides=(2, 2), padding='same'))
    model.add(LeakyReLU(alpha=0.2))

    model.add(Flatten())
    model.add(Dropout(0.4))
    model.add(Dense(1, activation='sigmoid'))

    return model

input_shape = (32, 32, 3)

def define_gan(generator, discriminator):
    discriminator.trainable = False
    model = Sequential()
    model.add(generator)
    model.add(discriminator)
    return model

# Initialize and compile models
latent_dim = 32 * 32 * 3
generator = define_generator()
discriminator = define_discriminator()
gan_model = define_gan(generator, discriminator)

# Create the GAN model
gan_model = define_gan(generator, discriminator)

# Compile the GAN model
gan_model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0002, beta_1=0.5))

discriminator.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0002, beta_1=0.5))
gan_model.compile(loss='binary_cross_entropy', optimizer=Adam(learning_rate=0.0002, beta_1=0.5))

def train_gan(gan_model, generator, discriminator, x_train, y_train, n_epochs=200, n_batch=1):
    half_batch = int(n_batch / 2)

    for epoch in range(n_epochs):
        # Select a random half batch of irregular images
        idx = np.random.randint(0, x_train.shape[0], half_batch)
        x_real = x_train[idx]
        y_real = y_train[idx]

        # Generate a batch of new regularized images
        x_fake = generator.predict(x_real)

        # Train the Discriminator with real images
        d_loss_real = discriminator.train_on_batch(y_real, np.ones((half_batch, 1)))

        # Train the Discriminator with fake images
        d_loss_fake = discriminator.train_on_batch(x_fake, np.zeros((half_batch, 1)))

        # Prepare a batch of data for the Generator
        x_gan = x_real
        y_gan = np.ones((half_batch, 1))

        # Train the Generator (via the Discriminator's error)
        g_loss = gan_model.train_on_batch(x_gan, y_gan)

        # Print the progress
        if epoch % 10 == 0:
            d_loss = 0.5 * np.add(d_loss_real[0], d_loss_fake[0])
            d_acc = 100.0 * 0.5 * np.add(d_loss_real[1], d_loss_fake[1])
            print(f"{epoch} [D loss: {d_loss}, acc.: {d_acc}] [G loss: {g_loss}]")

# Train the GAN model
train_gan(gan_model, generator, discriminator, x_train, y_train, n_epochs=200, n_batch=1)

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Reshape, LeakyReLU, Conv2DTranspose, Conv2D, Flatten, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import BinaryCrossentropy
from matplotlib import pyplot as plt

# Define the Generator model
def define_generator():
    model = Sequential()
    model.add(Dense(256 * 4 * 4, input_dim=32*32*3))
    model.add(LeakyReLU(alpha=0.2))
    model.add(Reshape((4, 4, 256)))

    model.add(Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))
    model.add(LeakyReLU(alpha=0.2))

    model.add(Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))
    model.add(LeakyReLU(alpha=0.2))

    model.add(Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))
    model.add(LeakyReLU(alpha=0.2))

    model.add(Conv2D(3, (3, 3), activation='tanh', padding='same'))

    return model

# Define the Discriminator model
def define_discriminator(input_shape=(32, 32, 3)):
    model = Sequential()
    model.add(Conv2D(64, (3, 3), padding='same', input_shape=input_shape))
    model.add(LeakyReLU(alpha=0.2))

    model.add(Conv2D(128, (3, 3), strides=(2, 2), padding='same'))
    model.add(LeakyReLU(alpha=0.2))

    model.add(Conv2D(128, (3, 3), strides=(2, 2), padding='same'))
    model.add(LeakyReLU(alpha=0.2))

    model.add(Conv2D(256, (3, 3), strides=(2, 2), padding='same'))
    model.add(LeakyReLU(alpha=0.2))

    model.add(Flatten())
    model.add(Dropout(0.4))
    model.add(Dense(1, activation='sigmoid'))

    return model

# Define the GAN model
def define_gan(generator, discriminator):
    discriminator.trainable = False
    model = Sequential()
    model.add(generator)
    model.add(discriminator)
    return model

# Training function
def train_gan(gan_model, generator, discriminator, x_train, y_train, n_epochs=200, n_batch=1):
    half_batch = int(n_batch / 2)

    for epoch in range(n_epochs):
        # Select a random half batch of irregular images
        idx = np.random.randint(0, x_train.shape[0], half_batch)
        x_real = x_train[idx]
        y_real = y_train[idx]

        # Generate a batch of new regularized images
        x_fake = generator.predict(x_real)

        # Train the Discriminator with real images
        d_loss_real = discriminator.train_on_batch(y_real, np.ones((half_batch, 1)))

        # Train the Discriminator with fake images
        d_loss_fake = discriminator.train_on_batch(x_fake, np.zeros((half_batch, 1)))

        # Prepare a batch of data for the Generator
        x_gan = x_real
        y_gan = np.ones((half_batch, 1))

        # Train the Generator (via the Discriminator's error)
        g_loss = gan_model.train_on_batch(x_gan, y_gan)

        # Print the progress
        if epoch % 10 == 0:
            d_loss = 0.5 * np.add(d_loss_real[0], d_loss_fake[0])
            d_acc = 100.0 * 0.5 * np.add(d_loss_real[1], d_loss_fake[1])
            print(f"{epoch} [D loss: {d_loss}, acc.: {d_acc}] [G loss: {g_loss}]")

# Initialize and compile models
latent_dim = 32 * 32 * 3
generator = define_generator()
discriminator = define_discriminator()
gan_model = define_gan(generator, discriminator)

discriminator.compile(loss=BinaryCrossentropy(), optimizer=Adam(learning_rate=0.0002, beta_1=0.5))
gan_model.compile(loss=BinaryCrossentropy(), optimizer=Adam(learning_rate=0.0002, beta_1=0.5))

# Load your images
# Assume x_train and y_train are already loaded and preprocessed
# x_train = ...
# y_train = ...

# Adjust batch size to match the small dataset
n_batch = 1  # Set to 1 or adjust according to dataset size

# Train the GAN model
train_gan(gan_model, generator, discriminator, x_train, y_train, n_epochs=200, n_batch=n_batch)

# Save the trained models
# generator.save('/content/generator_model.h5')
# discriminator.save('/content/discriminator_model.h5')
# gan_model.save('/content/gan_model.h5')

# # Test the GAN
# def test_gan(generator, x_test, y_test):
#     # Generate images from the test set
#     generated_images = generator.predict(x_test)

#     # Rescale generated images from [-1, 1] to [0, 255]
#     generated_images = (generated_images * 127.5) + 127.5
#     generated_images = generated_images.astype(np.uint8)

#     # Plot and compare some test images with their corresponding generated images
#     n_samples = min(10, x_test.shape[0])
#     plt.figure(figsize=(20, 10))
#     for i in range(n_samples):
#         plt.subplot(2, n_samples, i+1)
#         plt.axis('off')
#         plt.title('Original')
#         plt.imshow((y_test[i] * 127.5 + 127.5).astype(np.uint8))

#         plt.subplot(2, n_samples, n_samples+i+1)
#         plt.axis('off')
#         plt.title('Generated')
#         plt.imshow(generated_images[i])
#     plt.show()

# # Load the generator model
# from tensorflow.keras.models import load_model
# generator = load_model('/content/generator_model.h5')

# # Test the GAN
# test_gan(generator, x_test, y_test)

def test_gan(generator, x_test, y_test):
    # Generate images from the test set
    generated_images = generator.predict(x_test)

    # Rescale generated images from [-1, 1] to [0, 255]
    generated_images = (generated_images * 127.5) + 127.5
    generated_images = generated_images.astype(np.uint8)

    # Plot and compare some test images with their corresponding generated images
    n_samples = min(10, x_test.shape[0])
    plt.figure(figsize=(20, 10))
    for i in range(n_samples):
        plt.subplot(2, n_samples, i+1)
        plt.axis('off')
        plt.title('Original')
        plt.imshow((y_test[i] * 127.5 + 127.5).astype(np.uint8))

        plt.subplot(2, n_samples, n_samples+i+1)
        plt.axis('off')
        plt.title('Generated')
        plt.imshow(generated_images[i])
    plt.show()

# Test the GAN
test_gan(generator, x_test, y_test)

from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Reshape
from keras.layers import Conv2D
from keras.layers import Conv2DTranspose
from keras.layers import LeakyReLU
from keras.utils import plot_model

def define_generator(latent_dim):
  model = Sequential()
  # Foundation for 4x4 image
  n_nodes = 256*4*4
  model.add(Dense(n_nodes, input_dim = latent_dim))
  model.add(LeakyReLU(alpha = 0.2))
  model.add(Reshape((4,4,256)))

  #Upsample to 8x8
  model.add(Conv2DTranspose(128, (4,4), strides = (2,2), padding = 'same'))
  model.add(LeakyReLU(alpha = 0.2))

  #Upsample to 16x16

  model.add(Conv2DTranspose(128, (4,4), strides = (2,2) , padding = 'same'))
  model.add(LeakyReLU(alpha = 0.2))

  #Upsample to 32x32

  model.add(Conv2DTranspose(128, (4,4), strides = (2,2), padding = 'same'))
  model.add(LeakyReLU(alpha = 0.2))

  #output layer
  model.add(Conv2D(3, (3,3), activation = 'tanh', padding = 'same'))

  return model

latent_dim = 100
model = define_generator(latent_dim)

model.summary()

plot_model(model, to_file='generator_plot.png', show_shapes = True, show_layer_names = True)

#Generate n_samples for input to the generator to generate fake images
def generate_latent_points(latent_dim, n_samples):

  x_input = np.random.randn(latent_dim * n_samples)

  x_input = x_input.reshape(n_samples, latent_dim)

  return x_input

x_input = generate_latent_points(100, 64)

x_input.shape

def generate_fake_samples(g_model, latent_dim, n_samples):
  x_input = generate_latent_points(latent_dim, n_samples)

  X = g_model.predict(x_input)

  y = np.zeros((n_samples, 1))

  return X, y

latent_dim = 100

model = define_generator(latent_dim)

n_samples = 49

X, _ = generate_fake_samples(model, latent_dim, n_samples)

X = (X + 1) / 2.0

for i in range(n_samples):
  plt.subplot(7, 7, 1 + i)

  plt.axis('off')

  plt.imshow(X[i])

plt.show()

def define_gan(g_model, d_model):
  d_model.trainable = False

  model = Sequential()
  model.add(g_model)
  model.add(d_model)

  opt = Adam(lr = 0.0002, beta_1 = 0.5)
  model.compile(loss = 'binary_crossentropy', optimizer = opt)
  return model

latent_dim = 100
d_model = define_discriminator()

g_model = define_generator(latent_dim)

gan_model =define_gan(g_model, d_model)

gan_model.summary()

plot_model(gan_model, to_file = "gan_plot.png", show_shapes = True, show_layer_names = True)

def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs = 10, n_batch = 128):
  bat_per_epo = int(dataset.shape[0]/ n_batch)
  half_batch = int(n_batch/2)

  for i in range(n_epochs):
    for j in range(bat_per_epo):
      x_real, y_real = generate_real_samples(dataset, half_batch)

      d_loss1, _ = d_model.train_on_batch(x_real, y_real)

      x_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)

      d_loss2, _ = d_model.train_on_batch(x_fake, y_fake)

      x_gan = generate_latent_points(latent_dim, n_batch)

      y_gan = np.ones((n_batch, 1))

      g_loss = gan_model.train_on_batch(x_gan, y_gan)

      print("%d %d/%d, d1 = %.3f, d2 = %.3f g = %.3f" % (i+1, j+1, bat_per_epo, d_loss1, d_loss2, g_loss))

    if(i+1) %10 == 0:
      summarize_performance(i, g_model, d_model, dataset, latent_dim)

def summarize_performance(epoch, g_model, d_model, dataset, latent_dim, n_samples = 150):
  x_real, y_real = generate_real_samples(dataset, n_samples)

  _, acc_real = d_model.evaluate(x_real, y_real, verbose = 0)

  x_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_samples)

  _, acc_fake = d_model.evaluate(x_fake, y_fake, verbose = 0)

  print("Accuracy real: %.1f%% , fake: %.1f%%" % (acc_real * 100, acc_fake * 100))


  save_plot(x_fake, epoch)

  filename = "generator_model_%03d.h5" % (epoch + 1)

  g_model.save(filename)

def save_plot(examples, epoch, n = 7):
  examples = (examples + 1) / 2.0

  for i in range ( n * n):
    plt.subplot(n, n, i+1)
    plt.axis('off')
    plt.imshow(examples[i])
  filename = "generated_plot_e%03d.png" % (epoch + 1)
  plt.savefig(filename)
  plt.close()

train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs = 10, n_batch = 128)

from keras.models import load_model
from numpy.random import randn
import matplotlib.pyplot as plt

def generate_latent_points(latent_dim, n_samples):
  x_input = randn(latent_dim * n_samples)

  x_input = x_input.reshape(n_samples, latent_dim)
  return x_input

def create_plot(examples, n):
  for i in range(n * n):
    plt.subplot(n, n, i+1)
    plt.axis('off')
    plt.imshow(examples[i, :, :])
  plt.show()

model = load_model('generator_model_200.h5')

latent_points = generate_latent_points(100, 100)

X = model.predict(latent_points)

X = (X + 1) / 2.0

create_plot(X, 1)

X.shape[0]
